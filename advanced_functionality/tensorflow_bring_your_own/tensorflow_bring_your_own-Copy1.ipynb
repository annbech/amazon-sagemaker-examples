{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a TensorFlow Container for Your Training and Hosting Algorithms\n",
    "\n",
    "To use your own algorithms to train or deploy a model in Amazon SageMaker, package them in Docker containers. By packaging an algorithm in a container, you can use almost any code with Amazon SageMaker, regardless of the programming language, environment, framework, or dependencies. This example shows how to build a TensorFlow Docker container and use it for training and inference in Amazon SageMaker.\n",
    "\n",
    "1. [Building a TensorFlow Container for Your Training and Hosting Algorithms](#Building-a-Tensorflow-Container-for-Your_Training-and-Hosting-Algorithms)\n",
    "  1. [When Should You Create a Container for Your Algorithm?](#When-Should-You-Create-a-Container-for-Your-Algorithm?)\n",
    "  1. [Required Permissions](#Required-Permissions)\n",
    "  1. [The Example](#The-Example)\n",
    "  1. [The Presentation](#The-Presentation)\n",
    "1. [Part 1: Packaging and Uploading Your Algorithm for Use with Amazon SageMaker](#Part-1:-Packaging-and-Uploading-Your-Algorithm-for-Use-with-Amazon-SageMaker)\n",
    "    1. [Docker Overview](#Docker-Overview)\n",
    "    1. [How Amazon SageMaker Runs Your Docker Container](#How-Amazon-SageMaker-Runs-Your-Docker-Container)\n",
    "      1. [How Amazon SageMaker Runs Your Container During Training](#How-Amazon-SageMaker-Runs-Your-Container-During-Training)\n",
    "        1. [The Input](#The-Input)\n",
    "        1. [The Output](#The-Output)\n",
    "      1. [How Amazon SageMaker Runs Your Container During Hosting](#How-Amazon-SageMaker-Runs-Your-Container-During-Hosting)\n",
    "    1. [The Example Container](#The-Example-Container)\n",
    "    1. [The Dockerfile](#The-Dockerfile)\n",
    "    1. [Build and Register the Container](#Build-and-Register-the-Container)\n",
    "  1. [Test Your Algorithm Locally](#Test-Your-Algorithm-Locally)\n",
    "  1. [Download the CIFAR-10 Dataset](#Download-the-CIFAR-10-Dataset)\n",
    "  1. [Train Locally with the Amazon SageMaker Python SDK](#Train-Locally-with-the-Amazon-SageMaker-Python-SDK)\n",
    "  1. [Fit, Deploy, and Predict](#Fit,-Deploy,-and-Predict)\n",
    "  1. [Make Predictions Using the Amazon SageMaker Python SDK](#Make-Predictions-Using-the-Amazon-SageMaker-Python-SDK)\n",
    "1. [Part 2: Training and Hosting Your Algorithm in Amazon SageMaker](#Part-2:-Training-and-Hosting-Your-Algorithm-in-Amazon-SageMaker)\n",
    "  1. [Set Up the Environment](#Set-Up-the-Environment)\n",
    "  1. [Create the Session](#Create-the-Session)\n",
    "  1. [Upload the Data for Training](#Upload-the-Data-for-Training)\n",
    "  1. [Training on Amazon SageMaker](#Training-on--Amazon-SageMaker)\n",
    "  1. [Optional: Clean Up](#Optional:-Clean Up)  \n",
    "1. [Reference](#Reference)\n",
    "\n",
    "_or_ I'm impatient, just [let me see the code](#The-Dockerfile)!\n",
    "\n",
    "### When Should You Create a Container for Your Algorithm?\n",
    "\n",
    "It's not always necessary to create a container to use your own code in Amazon SageMaker. If you use a framework that's supported by Amazon SageMaker, such as Apache MXNet or TensorFlow, you can use the SDK entry points for that framework to supply the Python code that implements your algorithm. We regularly expand the set of supported frameworks, so always check to see if the framework that your algorithm was written in is supported.\n",
    "\n",
    "However, even if there is SDK support for your framework, sometimes it's more effective to build your own container. You might want to build your own container if the code that implements your algorithm is complex or if you need to make additions to the framework.\n",
    "\n",
    "If your framework is supported, you might consider building your own container for the following reasons:\n",
    "\n",
    "* A specific version isn't supported.\n",
    "* You need to configure and install your dependencies and environment.\n",
    "* You use a different training or hosting solution than the one provided.\n",
    " \n",
    "\n",
    "### Required Permissions\n",
    "\n",
    "This notebook creates new repositories in Amazon Elastic Container Registry (Amazon ECR), so you need permissions beyond those granted by the `SageMakerFullAccess` permissions to run it. To add these permissions, add the `AmazonEC2ContainerRegistryFullAccess` managed policy to the AWS Identity and Access Management (IAM) role that you used to start your notebook instance. The new permissions are available immediately (you don't need to restart your notebook instance).\n",
    "\n",
    "### The Example\n",
    "\n",
    "In this example, we show how to package a custom TensorFlow container with an example written in Python. The example uses the CIFAR-10 dataset for training and TensorFlow Serving for inference. You can use another inference solution by modifying the Docker container.\n",
    "\n",
    "We use a single image for both training and hosting because it's easier to manage one image. If training and hosting have different requirements, you might want to create a separate Dockerfile for each, then build two images. Choose the approach that is easier to develop and manage.\n",
    "\n",
    "If you're using Amazon SageMaker only for training or only for hosting, build only the required functionality into your container.\n",
    "\n",
    "[CIFAR-10]: http://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "### The Presentation\n",
    "\n",
    "This example is divided into two parts. The first explains how to _build_ the container and the second explains how to _use_ the container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Packaging and Uploading Your Algorithm for Use with Amazon SageMaker\n",
    "\n",
    "### Docker Overview\n",
    "\n",
    "If you're familiar with Docker, you can skip to [How Amazon SageMaker Runs Your Docker Container] (#How-Amazon-SageMaker-runs-your-Docker-container).\n",
    "\n",
    "Docker has become very popular in programming and devops communities because of its flexibility and its well-defined specification for how code can be run in its containers. It is the underpinning of many services built in the past few years, such as [Amazon Elastic Container Service (Amazon ECS)]. Although Docker containers are unfamiliar to many data scientists, they aren't difficult to build and use, and they can significantly simplify software package deployment. \n",
    "\n",
    "You use Docker to package arbitrary code into an _image_ that is totally self-contained. After creating the image, you use Docker to run a _container_ based on that image. Running a container is just like running a program, except that a container creates a fully self-contained environment for the program to run in. Containers are isolated from each other and from the host environment, so they run your program the way it is set up, no matter where you run it.\n",
    "\n",
    "Docker is more powerful than environment managers like Conda or virtualenv because it is completely language independent, and because it comprises your whole operating environment, including startup commands, and environment variable.\n",
    "\n",
    "A Docker container is like a virtual machine, but it is much lighter weight. For example, a program running in a container can start in less than a second, and many containers can run simultaneously on the same physical or virtual machine instance.\n",
    "\n",
    "Docker uses a simple file called a `Dockerfile` to specify how the image is assembled. You'll see an example in this walkthrough. You can build your Docker images based on Docker images that you've already built or on images built by others.\n",
    "\n",
    "Amazon SageMaker uses Docker to enable users to train and deploy arbitrary algorithms. In Amazon SageMaker, Docker containers are invoked one way for training and another, slightly different, way for hosting. \n",
    "\n",
    "For more information about Docker, see the following:\n",
    "\n",
    "* [Docker home page](http://www.docker.com)\n",
    "* [Getting started with Docker](https://docs.docker.com/get-started/)\n",
    "* [Dockerfile reference](https://docs.docker.com/engine/reference/builder/)\n",
    "* [`docker run` reference](https://docs.docker.com/engine/reference/run/)\n",
    "\n",
    "[Amazon ECS]: https://aws.amazon.com/ecs/\n",
    "\n",
    "### How Amazon SageMaker Runs Your Docker Container\n",
    "\n",
    "Because you can run the same image in training or hosting, Amazon SageMaker runs your container with the argument `train` or `serve`. How your container processes this argument depends on the container.\n",
    "\n",
    "* In this example, you don't define an `ENTRYPOINT` in the Dockerfile, so Docker runs the command [`train` at training time](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html) and [`serve` at serving time](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html). In this example, you define these as executable Python scripts, but you could use any program that you want to start in that environment.\n",
    "* If you specify a program as an `ENTRYPOINT` in the Dockerfile, that program runs at startup, and its first argument is `train` or `serve`. The program looks at that argument and decides what to do.\n",
    "* If you are building separate containers for training and hosting (or building for only one or the other), you can define a program as an `ENTRYPOINT` in the Dockerfile and ignore (or verify) the first argument that is passed in. \n",
    "\n",
    "#### How Amazon SageMaker Runs Your Container During Training\n",
    "\n",
    "When Amazon SageMaker runs training, your `train` script runs as in a regular Python program. The `/opt/ml` directory includes the following files:\n",
    "\n",
    "    /opt/ml\n",
    "    ├── input\n",
    "    │   ├── config\n",
    "    │   │   ├── hyperparameters.json\n",
    "    │   │   └── resourceConfig.json\n",
    "    │   └── data\n",
    "    │       └── <channel_name>\n",
    "    │           └── <input data>\n",
    "    ├── model\n",
    "    │   └── <model files>\n",
    "    └── output\n",
    "        └── failure\n",
    "\n",
    "##### The Input\n",
    "Input files provide the following:\n",
    "* `/opt/ml/input/config` contains information to control how your program runs. `hyperparameters.json` is a JSON-formatted dictionary that maps hyperparameter names to values. These values are always strings, so you might need to convert them. `resourceConfig.json` is a JSON-formatted file that describes the network layout used for distributed training.\n",
    "* `/opt/ml/input/data/<channel_name>/` (for File mode) contains the input data for that channel. The channels are created based on the call to the CreateTrainingJob operation, but it's important that channels match algorithm expectations. The files for each channel are copied from Amazon Simple Storage Service (Amazon S3) to this directory, preserving the tree structure indicated by the S3 key structure. \n",
    "* `/opt/ml/input/data/<channel_name>_<epoch_number>` (for Pipe mode) is the pipe for a given epoch. Epochs start at zero and increment by one each time you read them. There is no limit to the number of epochs that you can run, but you must close each pipe before reading the next epoch.\n",
    "\n",
    "##### The Output \n",
    "There are two output directories: \n",
    "* `/opt/ml/model/` is the directory where you write the model that your algorithm generates. Your model can be in any format. It can be a single file or a whole directory tree. Amazon SageMaker packages files in this directory into a compressed tar archive file. This file is made available at the Amazon S3 location returned in the `DescribeTrainingJob` response.\n",
    "* `/opt/ml/output` is the directory where the algorithm can write a file `failure` that describes why the job failed. The contents of this file are returned in the `FailureReason` field of the `DescribeTrainingJob` response. For jobs that succeed, there is no reason to write this file because Amazon SageMaker ignores it.\n",
    "\n",
    "#### How Amazon SageMaker Runs Your Container During Hosting\n",
    "\n",
    "Hosting requires a very different model than training because hosting reponds to inference requests that come in through HTTP. In this example, we use [TensorFlow Serving](https://www.tensorflow.org/serving/), but you can customize the hosting solution. For an example, see [Python serving stack within the scikit learn example](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.ipynb).\n",
    "\n",
    "Amazon SageMaker hosting uses two URLs that are included in the container:\n",
    "\n",
    "* `/ping` receives `GET` requests from the infrastructure. If the container is accepting requests, your program returns 200.\n",
    "* `/invocations` is the endpoint that receives client inference `POST` requests. The format of the request and the response is up to the algorithm. If the client supplied `ContentType` and `Accept` headers, these are passed in too. \n",
    "\n",
    "In the container, the model files are in the same place that they were written to during training:\n",
    "\n",
    "    /opt/ml\n",
    "    └── model\n",
    "        └── <model files>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Example Container\n",
    "\n",
    "The `container` directory contains all of the components that you need to package the example algorithm for Amazon SageMager:\n",
    "\n",
    "    .\n",
    "    ├── Dockerfile\n",
    "    ├── build_and_push.sh\n",
    "    └── cifar10\n",
    "        ├── cifar10.py\n",
    "        ├── resnet_model.py\n",
    "        ├── nginx.conf\n",
    "        ├── serve\n",
    "        ├── train\n",
    "\n",
    "The components perform the following tasks:\n",
    "\n",
    "* __`Dockerfile`__ describes how to build your Docker container image. \n",
    "* __`build_and_push.sh`__ is a script that uses the Dockerfile to build your container images and then pushes it to Amazon ECR. You invoke the commands directly later in this notebook, but you can copy and run the script for your own algorithms.\n",
    "* __`cifar10`__ contains the files that are installed in the container.\n",
    "\n",
    "For this simple application, you install only five files in the container: \n",
    "\n",
    "* __`cifar10.py`__ is the program that implements your training algorithm.\n",
    "* __`resnet_model.py`__ is the program that contains your ResNet model. \n",
    "* __`nginx.conf`__ is the configuration file for the nginx front end. Generally, you should be able to take this file as is.\n",
    "* __`serve`__ is the program that is started when the container is started for hosting. It launches nginx and loads your exported model with TensorFlow Serving.\n",
    "* __`train`__ is the program that is invoked when the container is run for training. Our implementation of this script invokes cifar10.py with hyperparameter values retrieved from /opt/ml/input/config/hyperparameters.json. We do this to avoid having to modify the training algorithm program.\n",
    "\n",
    "You might need only five files, but if you have many supporting routines, you might want to install more. \n",
    "\n",
    "This is the standard structure of our Python containers, although you are free to choose a different toolset and, therefore, could have a different layout. If you're writing in a different programming language, your layout will depend on the framework and tools that you choose.\n",
    "\n",
    "You probably will want to change two files for your application: `train` and `serve`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dockerfile\n",
    "\n",
    "The Dockerfile describes the image that you want to build. It describes the complete operating system installation of the system that you want to run. A running Docker container is much lighter than a full operating system, because it uses Linux on the host machine for basic operations. \n",
    "\n",
    "For the Python science stack, start with an official TensorFlow Docker image and run the standard tools to install TensorFlow Serving. Then add the code that implements your specific algorithm to the container, and set up the right environment for it to run under.\n",
    "\n",
    "Here's the Dockerfile for this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright 2017-2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\r\n",
      "#\r\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\"). You\r\n",
      "# may not use this file except in compliance with the License. A copy of\r\n",
      "# the License is located at\r\n",
      "#\r\n",
      "#     http://aws.amazon.com/apache2.0/\r\n",
      "#\r\n",
      "# or in the \"license\" file accompanying this file. This file is\r\n",
      "# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\r\n",
      "# ANY KIND, either express or implied. See the License for the specific\r\n",
      "# language governing permissions and limitations under the License.\r\n",
      "\r\n",
      "# For more information on creating a Dockerfile\r\n",
      "# https://docs.docker.com/compose/gettingstarted/#step-2-create-a-dockerfile\r\n",
      "FROM tensorflow/tensorflow:1.8.0-py3\r\n",
      "\r\n",
      "RUN apt-get update && apt-get install -y --no-install-recommends nginx curl\r\n",
      "\r\n",
      "# Download TensorFlow Serving\r\n",
      "# https://www.tensorflow.org/serving/setup#installing_the_modelserver\r\n",
      "RUN echo \"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | tee /etc/apt/sources.list.d/tensorflow-serving.list\r\n",
      "RUN curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -\r\n",
      "RUN apt-get update && apt-get install tensorflow-model-server\r\n",
      "\r\n",
      "ENV PATH=\"/opt/ml/code:${PATH}\"\r\n",
      "\r\n",
      "# /opt/ml and all subdirectories are utilized by SageMaker, we use the /code subdirectory to store our user code.\r\n",
      "COPY /cifar10 /opt/ml/code\r\n",
      "WORKDIR /opt/ml/code"
     ]
    }
   ],
   "source": [
    "!cat container/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Register the Container\n",
    "\n",
    "The following shell code uses `docker build` to build the container image and `docker push` to push the container image to Amazon ECR. This code is also available as the shell script `container/build-and-push.sh`, which you can run as `build-and-push.sh tensorflow-cifar10-example` to build the image `tensorflow-cifar10-example`. \n",
    "\n",
    "The code looks for an Amazon ECR repository in the account that you're using and the current default AWS Region (if you're using an Amazon SageMaker notebook instance, this is the Region where the notebook instance was created). If there is no Amazon ECR repository, the script creates it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=tensorflow-cifar10-example\n",
    "\n",
    "cd container\n",
    "\n",
    "chmod +x cifar10/train\n",
    "chmod +x cifar10/serve\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build  -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Algorithm Locally \n",
    "\n",
    "When you're packaging your first algorithm for use with Amazon SageMaker, it's a good idea to test it to make sure it's working correctly. You use the [Amazon SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk) to test both locally and on Amazon SageMaker. To test our algorithm, you need to download our dataset.\n",
    "\n",
    "For more examples of using the Amazon SageMaker Python SDK, see [Amazon SageMaker Examples](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the CIFAR-10 Dataset\n",
    "The training algorithm expects training data to be in [TFRecords](https://www.tensorflow.org/guide/datasets) file format. TFRecords format is a simple record-oriented binary format that many TensorFlow applications use for training data.\n",
    "The following Python script downloads the CIFAR-10 dataset and converts them into TFRecords. It is adapted from the [official TensorFlow CIFAR-10 example](https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python utils/generate_cifar10_tfrecords.py --data-dir=/tmp/cifar-10-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval.tfrecords\ttrain.tfrecords  validation.tfrecords\r\n"
     ]
    }
   ],
   "source": [
    "# There should be three tfrecords. (eval, train, validation)\n",
    "! ls /tmp/cifar-10-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Locally with the Amazon SageMaker Python SDK \n",
    "To represent training, you use the Estimator class. You need to configure the following: \n",
    "1. IAM role - The AWS execution role.\n",
    "2. train_instance_count - The number of instances to use for training.\n",
    "3. train_instance_type - The type of instance to use for training. For training locally, specify `local`.\n",
    "4. image_name - The name of the custom TensorFlow Docker image that we created.\n",
    "5. hyperparameters - The hyperparameters that we want to pass.\n",
    "\n",
    "To set up the IAM role, you use a helper function in the Amazon SageMaker Python SDK. The function gets metadata from the notebook instance, so if you run it outside of a notebook instance, it throws an exception. To run the function outside of a notebook instance, you must provide an IAM role that has the required permissions. For more information, see [Permissions](#Permissions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit, Deploy, and Predict\n",
    "\n",
    "Now you can call `fit()` with the path to your local copy of the CIFAR-10 dataset prefixed with `file://`. This invokes the TensorFlow container with `train` and passes in your hyperparameters and other metadata as .json files in /opt/ml/input/config within the container.\n",
    "\n",
    "After training succeeds, the training algorithm outputs a trained model to the /opt/ml/model directory, which is used to handle predictions.\n",
    "\n",
    "You can then call `deploy()` with the instance_count and instance_type, `1` and `local`, respectively. This invokes the TensorFlow container with `serve`, which sets up your container to handle prediction requests through TensorFlow Serving. A predictor is returned, which you use to make inferences against your trained model.\n",
    "\n",
    "After you get your prediction, you can delete the endpoint.\n",
    "\n",
    "We recommend testing and training your training algorithm locally first, because it iterates faster and is easier to debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker instance route table setup is ok. We are good to go.\r\n",
      "SageMaker instance routing for Docker is ok. We are good to go!\r\n"
     ]
    }
   ],
   "source": [
    "# Lets set up our SageMaker notebook instance for local mode.\n",
    "!/bin/bash ./utils/setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "hyperparameters = {'train-steps': 100}\n",
    "\n",
    "instance_type = 'local'\n",
    "\n",
    "estimator = Estimator(role=role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type=instance_type,\n",
    "                      image_name='tensorflow_cifar10_example:latest',\n",
    "                      hyperparameters=hyperparameters)\n",
    "\n",
    "estimator.fit('file:///tmp/cifar-10-data')\n",
    "\n",
    "predictor = estimator.deploy(1, instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions Using the Amazon SageMaker Python SDK\n",
    "\n",
    "To make predictions, you use an image that is converted into JSON format with OpenCV. You send this as an inference request. You also install OpenCV to deserialize the image that is used to make predictions.\n",
    "\n",
    "The JSON response is the probability that the image belongs to each of the 10 classes and the most likely class that it belongs to. For a list of classes, see the [CIFAR-10 website](https://www.cs.toronto.edu/~kriz/cifar.html). We didn't train the model for long, so we aren't expecting very accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-L58J2_1  |\u001b[0m 172.18.0.1 - - [03/Aug/2018:22:32:52 +0000] \"POST /invocations HTTP/1.1\" 200 229 \"-\" \"-\"\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'predictions': [{'probabilities': [2.29861e-05,\n",
       "    0.0104983,\n",
       "    0.147974,\n",
       "    0.01538,\n",
       "    0.0478089,\n",
       "    0.00164997,\n",
       "    0.758483,\n",
       "    0.0164191,\n",
       "    0.00125304,\n",
       "    0.000510801],\n",
       "   'classes': 6}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy\n",
    "\n",
    "from sagemaker.predictor import json_serializer, json_deserializer\n",
    "\n",
    "image = cv2.imread(\"data/cat.png\", 1)\n",
    "\n",
    "# resize, as our model is expecting images in 32x32.\n",
    "image = cv2.resize(image, (32, 32))\n",
    "\n",
    "data = {'instances': numpy.asarray(image).astype(float).tolist()}\n",
    "\n",
    "# The request and response format is JSON for TensorFlow Serving.\n",
    "# For more information: https://www.tensorflow.org/serving/api_rest#predict_api\n",
    "predictor.accept = 'application/json'\n",
    "predictor.content_type = 'application/json'\n",
    "\n",
    "predictor.serializer = json_serializer\n",
    "predictor.deserializer = json_deserializer\n",
    "\n",
    "# For more information on the predictor class.\n",
    "# https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/predictor.py\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Training and Hosting Your Algorithm in Amazon SageMaker\n",
    "After packaging your container, you can use it to train and serve models. \n",
    "\n",
    "### Set Up the Environment\n",
    "Specify the S3 bucket to use and the IAM role needed to work with Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 prefix\n",
    "prefix = 'DEMO-tensorflow-cifar10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Session\n",
    "\n",
    "The session remembers connection parameters to Amazon SageMaker. you perform all of our Amazon SageMaker operations in the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "\n",
    "sess = sage.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the Data for Training\n",
    "\n",
    "To upload the data to a default S3 bucket, you use the tools provided by the Amazon SageMaker Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIRECTORY = '/tmp/cifar-10-data'\n",
    "\n",
    "data_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on Amazon SageMaker\n",
    "To train a model on Amazon SageMaker, you use the Amazon SageMaker Python SDK similar to the way that you used it to train a model locally. You do the following:\n",
    "\n",
    "1. Change the train_instance_type from `local` to one of the [supported EC2 instance types](https://aws.amazon.com/sagemaker/pricing/instance-types/).\n",
    "2. Specify the URL of the Amazon ECR image that you pushed.\n",
    "3. Make sure that your local training dataset is in Amazon S3 and that the S3 URL to the dataset is passed into the `fit()` call.\n",
    "\n",
    "Begin by fetching the URL of the Amazon ECR image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client('sts')\n",
    "account = client.get_caller_identity()['Account']\n",
    "\n",
    "my_session = boto3.session.Session()\n",
    "region = my_session.region_name\n",
    "\n",
    "algorithm_name = 'tensorflow-cifar10-example'\n",
    "\n",
    "ecr_image = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account, region, algorithm_name)\n",
    "\n",
    "print(ecr_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "hyperparameters = {'train-steps': 100}\n",
    "\n",
    "instance_type = 'ml.m4.xlarge'\n",
    "\n",
    "estimator = Estimator(role=role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type=instance_type,\n",
    "                      image_name=ecr_image,\n",
    "                      hyperparameters=hyperparameters)\n",
    "\n",
    "estimator.fit(data_location)\n",
    "\n",
    "predictor = estimator.deploy(1, instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"data/cat.png\", 1)\n",
    "\n",
    "# resize, as our model is expecting images in 32x32.\n",
    "image = cv2.resize(image, (32, 32))\n",
    "\n",
    "data = {'instances': numpy.asarray(image).astype(float).tolist()}\n",
    "\n",
    "predictor.accept = 'application/json'\n",
    "predictor.content_type = 'application/json'\n",
    "\n",
    "predictor.serializer = json_serializer\n",
    "predictor.deserializer = json_deserializer\n",
    "\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Clean Up\n",
    "You can see all of the training jobs, models, and endpoints that you created in the Amazon SageMaker console in your AWS account. When you're done with the endpoint, delete it to avoid accruing unnecessary charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- [How Amazon SageMaker Runs Your Training Image](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html)\n",
    "- [How Amazon SageMaker Runs Your Inference Image](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html)\n",
    "- [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "- [Amazon SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk)\n",
    "- [Dockerfile](https://docs.docker.com/engine/reference/builder/)\n",
    "- [scikit-bring-your-own](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
